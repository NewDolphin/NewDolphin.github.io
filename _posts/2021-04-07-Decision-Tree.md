---
layout:     post
title:      决策树
subtitle:   
date:       2021-04-07
author:     XP
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],
            displayMath: [["$$", "$$"], ["\\[", "\\]"]]
            }
        });
    </script>
</head>


# 决策树

## ID3算法(1986)
使用信息论中的熵来进行特征选择。熵用来衡量事物的不确定性，不确定性越大，熵越大。随机变量X的熵表达为 $H(X)=-\sum_{i=1}^{n}p_{i}\cdot log_{2}(p_{i})$，n代表X的n种不同的离散取值，$p_{i}$ 代表X取值为i的概率。

条件熵 $H(Y\mid X)$ 表示在已知随机变量X的条件下随机变量Y的不确定性。定义为：$H(Y\mid X)=\sum_{x\epsilon X}^{}p(x)\cdot H(Y\mid X=x)$。条件熵是指在给定某个数（某个变量为某个值）的情况下，另一个变量的不确定性(熵)是多少。

样本集合D的熵定义为 $Ent(D)=-\sum_{k=1}^{\left\| y \right\|}p_{k} \cdot log_{2}(p_{k})$，D中包含 $\left\| y \right\|$ 个类别，$p_{k}$ 表示第k类样本所占比例，$Ent(D)$ 值越小，则D的纯度越高。

设离散属性 $a$ 有V个可取值，则以属性 $a$ 划分会产生V个分支结点，每个分支结点包含的样本集和记为 $D^{v}$。以属性 $a$ 对样本集进行划分获得信息增益为
$$ Gain(D,a)=Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\cdot Ent(D^v)  $$ 
ID3算法以*信息增益*来选择每一次的划分属性

### 算法的过程为：
设输入包含m个样本(样本集合D)，每个样本有n个离散特征(特征集合A)，输出为决策树T。

1. 初始化信息增益的阈值 $\epsilon$
2. 判断样本集合是否为同一类，如果是则返回单结点树T，返回类别
3. 判断特征是否为空，如果是则返回单结点树T，标记类别为样本中输出类别D实例数最多的类别。
4. 计算特征集和A中各个特征对输出D的信息增益，选择信息增益最大的特征 $a^g$
5. 如果 $a^g$ 的信息增益小于阈值 $\epsilon$，则返回单结点树T，标记类别为样本中实例数最多的对应类别。
6. 否则，按属性a的不同取值将对应的样本输出D分成不同的类别 $D^v$。每个类别产生一个子结点。返回增加了结点的树T。
7. 对于所有的子结点，令 $D=D^v,A=A−{a^g}$ 递归调用2-6步，得到子树 $T^i$ 并返回。

### ID3算法的不足
1. 优先使用信息增益大的特征，使其倾向于选择取值多的属性，例如ID
2. 没有考虑连续属性，比如长度、密度等连续值。
3. 没有考虑缺失值的情况
4. 没有考虑过拟合的问题，没有剪枝

注：ID3算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。为什么不叫ID4，ID5之类的名字呢?那是因为决策树太火爆，他的ID3一出来，别人二次创新，很快 就占了ID4， ID5，所以他另辟蹊径，取名C4.0算法，后来的进化版为C4.5算法

## C4.5算法(1993)
昆兰在C4.5算法中改进了上述4个问题。

### 属性选择标准
C4.5算法使用信息增益率来选择最优的划分属性，信息增益率定义为：
$$ Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}  $$
，其中
$$ IV(a)=-\sum_{v=1}^{V}\frac{\left\| D^v \right\|}{\left\| D \right\|}\cdot log_2\frac{\left\| D^v \right\|}{\left| D \right\|} $$
称为属性a的固有值，属性a的可取值越多，IV(a)就越大。

信息增益率对可取值数较少的属性有偏好，因此C4.5算法并不是直接选择增益率最大的属性，而是先从属性中选择信息增益高于平均水平的属性，再从中选择增益率最高的。

### 连续属性
使用二分法，将离散特征连续化。比如连续属性a有m个取值，从小到大排列，可以得到m-1个划分点 $T_i=\frac{a_i+a_{i+1}}{2}$，计算这个m-1个划分点作为二元分类时的信息增益率，最大的作为该属性可获得的信息增益率。与离散属性不同，连续属性还可以作为后代结点的划分属性。

### 缺失值
要解决两个问题：
1. 如何在属性值缺失时对属性进行选择
2. 给定划分属性，若某样本在该属性上值缺失，如何对该样本进行划分

#### 问题1
给定训练集D和属性a，令 $\tilde{D}$ 表示D中在属性a上无缺失的样本集，我们仅可以通过 $\tilde{D}$ 判断属性a的优劣。设属性a有V个可取值，令 $\tilde{D}^v$ 表示$\tilde{D}$ 在属性a上取值为 $a^v$ 的样本子集。$\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第k类的样本子集。我们为每个样本x设定一个权重 $w_x$，初始时为1。定义：

$\rho$ 为无缺失值样本所占比例：$\rho= \frac{\sum_{x\in\tilde{D}}^{}w_x}{\sum_{x\in D}^{}w_x}$

$\tilde{p_k}$ 表示无缺失值样本中第k类所占比例：$\tilde{p_k}= \frac{\sum_{x\in\tilde{D_k}}^{}w_x}{\sum_{x\in \tilde{D}}^{}w_x}$

$\tilde{r_v}$ 表示无缺失值样本中在属性a上取值 $a^v$ 的样本所占的比例：$\tilde{r_v}= \frac{\sum_{x\in\tilde{D^v}}^{}w_x}{\sum_{x\in \tilde{D}}^{}w_x}$

基于上述定义，信息增益的计算方式可以推广为
$$ Gain(D,a)= \rho\cdot Gain(\tilde{D},a)=\rho\cdot (Ent(\tilde{D})-\sum_{v=1}^{V}\tilde{r_v}\cdot Ent(\tilde{D^v})) $$
其中 $Ent(\tilde{D})=-\sum_{k=1}^{\left| y \right|}\tilde{p_k}\cdot log_2(\tilde{p_k})$

#### 问题2
若样本x在属性a取值已知，则将x划分入对应结点，且保留权重 $w_x$。若未知，则将样本x划入全部子结点，且在不同子结点中的权重值调整为 $\tilde{r_v}\cdot w_x$。直观地看，就是让有缺失值的样本以不同概率划分入不同的子结点。（样例见"西瓜书P87"）

### 剪枝
1. 预剪枝，在生成决策树的时候就决定是否剪枝。
2. 后剪枝，先生成决策树，再通过交叉验证来剪枝。

### C4.5算法的问题
1. 生成的是多叉树，即一个父结点可以有多个结点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
2. 只能用于分类
3. 由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。

## CART算法(1984)
CART算法既可以做回归，也可以做分类。scikit-learn的决策树使用的也是CART算法。

### 最优属性选择方法
“信息增益”会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？CART算法使用基尼系数来代替信息增益比。基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，属性越好。

给定一个数据集D，基尼值的表达式为
$$ Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\ne k}^{}p_k \cdot p_{k'}=1-\sum_{k=1}^{|y|}p_k^2 $$
$Gini(D)$ 反映了从数据集D中随机抽两个样本，其类别不一致的概率，$Gini(D)$ 越小，则数据集D的纯度越高。属性a的基尼指数定义为
$$ Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)  $$
在候选属性集合A中，选择使得划分后基尼指数最小的属性作为划分属性。

对比基尼值表达式和熵的表达式，二次运算比对数简单很多。而且基尼值的对应的误差仅比熵稍差一点，因此，基尼值可以做为熵模型的一个近似替代。

为了进一步简化，CART分类树算法每次分支仅选择某个属性进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。

### 处理连续属性和离散属性
CART算法处理连续属性的方法与C4.5是一致的

CART算法处理离散属性的方法与C4.5不同，设属性a具有三个类别{a1,a2,a3}。会分三种二分的情况查看属性A作为划分属性的效果。{a1}和{a2,a3}；{a2}和{a1,a3}；{a3}和{a1,a2}，找到基尼指数最小的组合，作为属性a的基尼指数。由于这次没有把属性a的取值完全分开，后面我们还有机会在子结点继续选择到属性a来划分数据。这和ID3或者C4.5不同，离散特征只会参与一次结点的建立。

### CART分类树建立算法
输入：训练集D，基尼指数的阈值，样本个数阈值<br>
输出：决策树T。

1. 对于当前结点的数据集，如果样本个数小于阈值或者没有特征，则返回决策子树，当前结点停止递归。
2. 计算样本集D的基尼指数，如果基尼系数小于阈值，则返回决策树子树，当前结点停止递归。
3. 计算当前结点现有的各个属性的各个值对数据集D的基尼指数。
4. 选择基尼指数最小的属性a和对应的特征值 $a^v$。根据这个最优属性和最优属性值，把数据集划分成两部分D1和D2。
5. 对左右的子结点递归的调用1-4步，生成决策树。

使用生成的决策树做预测的时候，假如测试集里的样本x落到了某叶子结点，而结点里有多个训练样本。则对于A的类别预测采用的是这个叶子结点里概率最大的类别。

### CART回归树建立算法
CART分类树和回归树区别主要有两点

#### 选择最优划分属性的方式不同
CART算法使用了和方差来计算每个属性值作为划分属性时的效果。具体来说，使用任意划分属性a，将D划分成的数据集D1和D2，求出D1和D2的均方差之和，表达式为：
$$ \sum_{x_i\in D_1}^{}(y_i-c_1)^2 +  \sum_{x_i\in D_2}^{}(y_i-c_2)^2  $$
$c_1$ 和 $c_2$ 分别表示两个数据集的标签均值

#### 预测的方式
CART回归树采用最终叶子的均值或者中位数来预测输出结果

### 剪枝
决策树算法很容易对训练集过拟合，导致泛化能力差。为了解决这个问题，我们需要对CART树进行剪枝，来增加决策树的泛化能力。

#### 预剪枝
在决策树生长过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分当前结点，并将当前结点标记为叶结点。具体来说比较 “不划分当前结点” 和 “以最佳属性划分当前结点” 时决策树在验证集上的表现。

预剪枝使得很多分支不会展开，减低了过拟合风险，也减少了训练时间。但使用预剪枝可能得不到最佳的决策树，比如存在基于这些未出现的分支的子分支可能会显著提高决策树泛化能力。预剪枝可能带来欠拟合的风险。

#### 后剪枝
先利用训练集生成一棵完整的决策树，计算完整决策树的验证集精度。再从最底层叶结点(同层由左至右)，自底向上尝试将以每一个内部结点为根的子树替换为叶结点(剪枝)时，树在验证集上的表现，如果表现更好，则执行剪枝，否则不剪枝。再尝试剪下一个结点时，以剪枝后的结果为基础。(西瓜书)

后剪枝的决策树欠拟合风险小，泛化能力往往优于预剪枝决策树。但是由于后剪枝是在决策树生成之后，自底向上依次对非叶结点进行观察，时间开销比预剪枝更大。

## 三种算法对比
| 算法 | 应用场景 | 树结构 | 特征选择方法 | 连续属性 | 缺失值处理 | 剪枝 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|ID3|分类|多叉树|信息增益|x|x|x|
|C4.5|分类|多叉树|信息增益率|v|v|v|
|CART|分类、回归|二叉树|基尼系数|v|v|v|

## 决策树总结
### 优点
1. 不需要预处理：归一化、处理缺失值。
2. 相比于神经网络之类的黑盒模型，决策树在逻辑上可以得到很好的解释
3. 对于异常点的容错能力好，健壮性高。

### 缺点
1. 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置结点最少样本数量和限制决策树深度来改进。
2. 样本集的一点改动，就可能导致决策树结构的剧烈变化。这个可以通过集成学习的方法解决。
3. 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。*
